##-------------------------------------------------#
# flume ng conf 
# flume ng的各种source/sink配置，已全部单机验证
#
# author: libo211321
##-------------------------------------------------#

# Describe the source 
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
###-------------------------------------------------#
## 使用avro rpc source
#a1.sources.r1.type = avro
#a1.sources.r1.bind = localhost
#a1.sources.r1.port = 44444
###-------------------------------------------------#

###-------------------------------------------------#
## 使用linux命令执行结果作为source
#a1.sources.r1.type = exec
#a1.sources.r1.command = /opt/hadoop-client/hadoop25-porsche/hadoop/bin/hadoop fs -cat /adta/datacenter/raw/imei2app/20160630/part-00000
###-------------------------------------------------#

###-------------------------------------------------#
## 使用spooldir source
#a1.sources.r1.type = spooldir
#a1.sources.r1.spoolDir = /search/libo/flume/apache-flume-1.6.0/test_source
#a1.sources.r1.fileHeader = true
###-------------------------------------------------#

###-------------------------------------------------#
## 使用kafka source
a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
a1.sources.r1.zookeeperConnect = rsync.jnode01.mystra.nm.ted:2181
a1.sources.r1.kafka.bootstrap.servers=rsync.node01.mystra.nm.ted:6667,rsync.node02.mystra.nm.ted:6667,rsync.node03.mystra.nm.ted:6667,rsync.node04.mystra.nm.ted:6667,rsync.node05.mystra.nm.ted:6667,rsync.node06.mystra.nm.ted:6667,rsync.node07.mystra.nm.ted:6667,rsync.node08.mystra.nm.ted:6667,rsync.node09.mystra.nm.ted:6667,rsync.node10.mystra.nm.ted:6667,rsync.node11.mystra.nm.ted:6667,rsync.node12.mystra.nm.ted:6667,rsync.node13.mystra.nm.ted:6667,rsync.node14.mystra.nm.ted:6667,rsync.node15.mystra.nm.ted:6667,rsync.node16.mystra.nm.ted:6667,rsync.node17.mystra.nm.ted:6667,rsync.node18.mystra.nm.ted:6667,rsync.node19.mystra.nm.ted:6667
a1.sources.r1.topic = weixin_article
a1.sources.r1.kafka.consumer.max.partition.fetch.bytes=20485760
a1.sources.r1.groupId = flumeTest
a1.sources.r1.kafka.consumer.timeout.ms = 100
###-------------------------------------------------#

###-------------------------------------------------#
## 使用kafka source
#a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
#a1.sources.r1.zookeeperConnect = localhost:12181
#a1.sources.r1.topic = flumeSourceTestTopic
#a1.sources.r1.groupId = flume
#a1.sources.r1.kafka.consumer.timeout.ms = 100
###-------------------------------------------------#


##-------------------------------------------------#
# 使用rocketmq source
## 将实时计算平台的一路输出为源
#a1.sources.r1.type=com.sogou.flume.rocketmq.adaptor.source.RocketmqSource
#a1.sources.r1.namesrvAddr=10.139.22.44:9876
#a1.sources.r1.consumerGroup=flume-group
#a1.sources.r1.topic=SELF_TEST_TOPIC
#a1.sources.r1.consumeFromWhere=CONSUME_FROM_MIN_OFFSET
##a1.sources.r1.tags=*
#a1.sources.r1.messageModel=CLUSTERING
#a1.sources.r1.maxNums=32
###-------------------------------------------------#

# Describe the sink
##-------------------------------------------------#
## 将数据输出到日志中（打印到屏幕）
#a1.sinks.k1.type = logger
##-------------------------------------------------#

##-------------------------------------------------#
## 将数据输出到本地文件中
## sink配置信息
## "file_roll"表示将数据存入本地文件系统
a1.sinks.k1.type = file_roll
## 指定数据存放目录
a1.sinks.k1.sink.directory = /search/libo/flume/apache-flume-1.6.0/test_dir/result
## 设置滚动时间(即每隔一段你设置的时间，系统会生成一个新的文件存放数据
## (如果不指定，系统会默认生成N个文件，将数据分别存入N个文件中),
## 为0时表示只有一个文件存放数据) 
a1.sinks.k1.sink.rollInterval = 0
##-------------------------------------------------#

#-------------------------------------------------#
# 将数据输出到hdfs
#a1.sinks.k1.type = hdfs
#a1.sinks.k1.hdfs.path = hdfs://rsync.porsche.hadoop.cxc.ad.nop.sogou-op.org/adta/libo/flume/sink/destination/dev
## Kerberos配置
#a1.sinks.k1.hdfs.kerberosPrincipal = adta
#a1.sinks.k1.hdfs.kerberosKeytab = /opt/hadoop-client/hadoop25-porsche/adta.keytab
#a1.sinks.k1.hdfs.writeFormat = Text
#a1.sinks.k1.hdfs.fileType = DataStream
#a1.sinks.k1.hdfs.rollInterval = 0
#a1.sinks.k1.hdfs.rollSize = 1000000
#a1.sinks.k1.hdfs.rollCount = 0
#a1.sinks.k1.hdfs.batchSize = 1000
#a1.sinks.k1.hdfs.txnEventMax = 1000
#a1.sinks.k1.hdfs.callTimeout = 60000
#a1.sinks.k1.hdfs.appendTimeout = 60000
##-------------------------------------------------#

##-------------------------------------------------#
## 将数据sink到kafka中
#a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink
#a1.sinks.k1.topic = flume2kafkaTopic
#a1.sinks.k1.brokerList = 127.0.0.1:19092,localhost:29092
#a1.sinks.k1.requiredAcks = 1
#a1.sinks.k1.batchSize = 20
##-------------------------------------------------#

###-------------------------------------------------#
## 将数据sink到rocketmq中
#a1.sinks.k1.type=com.sogou.flume.rocketmq.adaptor.sink.RocketmqSink
#a1.sinks.k1.namesrvAddr=10.139.22.44:9876
#a1.sinks.k1.producerGroup=ProducerGroup-flume
#a1.sinks.k1.topic=SELF_TEST_TOPIC
###-------------------------------------------------#

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 10000000
a1.channels.c1.transactionCapacity = 1000000

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1

