##-------------------------------------------------#
# flume ng conf 
# flume ng的各种source/sink配置，已全部单机验证
#
# author: libo211321
##-------------------------------------------------#

# Describe the source 
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
###-------------------------------------------------#
## 使用avro rpc source
#a1.sources.r1.type = avro
#a1.sources.r1.bind = localhost
#a1.sources.r1.port = 44444
###-------------------------------------------------#

###-------------------------------------------------#
## 使用linux命令执行结果作为source
#a1.sources.r1.type = exec
#a1.sources.r1.command = /opt/hadoop-client/hadoop25-porsche/hadoop/bin/hadoop fs -cat /adta/datacenter/raw/imei2app/20160630/part-00000
###-------------------------------------------------#

###-------------------------------------------------#
## 使用spooldir source
#a1.sources.r1.type = spooldir
#a1.sources.r1.spoolDir = /search/libo/flume/apache-flume-1.6.0/test_source
#a1.sources.r1.fileHeader = true
###-------------------------------------------------#

###-------------------------------------------------#
## 使用kafka source
#a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
#a1.sources.r1.zookeeperConnect = localhost:12181
#a1.sources.r1.topic = flumeSourceTestTopic
#a1.sources.r1.groupId = flume
#a1.sources.r1.kafka.consumer.timeout.ms = 100
###-------------------------------------------------#


##-------------------------------------------------#
# 使用rocketmq source
# 将实时计算平台的一路输出为源
a1.sources.r1.type=com.sogou.flume.rocketmq.adaptor.source.RocketmqSource
a1.sources.r1.namesrvAddr=10.139.22.44:9876
a1.sources.r1.consumerGroup=waplx_front_dev_consumer
a1.sources.r1.topic=waplx_front_src-waplx_front_emit_dmp
a1.sources.r1.consumeFromWhere=CONSUME_FROM_LAST_OFFSET
#a1.sources.r1.tags=*
a1.sources.r1.messageModel=CLUSTERING
a1.sources.r1.maxNums=32
##-------------------------------------------------#

# Describe the sink
##-------------------------------------------------#
## 将数据输出到日志中（打印到屏幕）
#a1.sinks.k1.type = logger
##-------------------------------------------------#

##-------------------------------------------------#
## 将数据输出到本地文件中
## sink配置信息
## "file_roll"表示将数据存入本地文件系统
#a1.sinks.k1.type = file_roll
## 指定数据存放目录
#a1.sinks.k1.sink.directory = /search/libo/flume/apache-flume-1.6.0/test_dir/result
## 设置滚动时间(即每隔一段你设置的时间，系统会生成一个新的文件存放数据
## (如果不指定，系统会默认生成N个文件，将数据分别存入N个文件中),
## 为0时表示只有一个文件存放数据) 
#a1.sinks.k1.sink.rollInterval = 0
##-------------------------------------------------#

#-------------------------------------------------#
# 将数据输出到hdfs
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = hdfs://rsync.porsche.hadoop.cxc.ad.nop.sogou-op.org/adta/libo/flume/sink/destination/dev
# Kerberos配置
a1.sinks.k1.hdfs.kerberosPrincipal = adta
a1.sinks.k1.hdfs.kerberosKeytab = /opt/hadoop-client/hadoop25-porsche/adta.keytab
a1.sinks.k1.hdfs.writeFormat = Text
a1.sinks.k1.hdfs.fileType = DataStream
a1.sinks.k1.hdfs.rollInterval = 0
a1.sinks.k1.hdfs.rollSize = 1000000
a1.sinks.k1.hdfs.rollCount = 0
a1.sinks.k1.hdfs.batchSize = 1000
a1.sinks.k1.hdfs.txnEventMax = 1000
a1.sinks.k1.hdfs.callTimeout = 60000
a1.sinks.k1.hdfs.appendTimeout = 60000
#-------------------------------------------------#

##-------------------------------------------------#
## 将数据sink到kafka中
#a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink
#a1.sinks.k1.topic = flume2kafkaTopic
#a1.sinks.k1.brokerList = 127.0.0.1:19092,localhost:29092
#a1.sinks.k1.requiredAcks = 1
#a1.sinks.k1.batchSize = 20
##-------------------------------------------------#

###-------------------------------------------------#
## 将数据sink到rocketmq中
#a1.sinks.k1.type=com.sogou.flume.rocketmq.adaptor.sink.RocketmqSink
#a1.sinks.k1.namesrvAddr=10.134.97.126:9876
#a1.sinks.k1.producerGroup=ProducerGroup-flume
#a1.sinks.k1.topic=SELF_TEST_TOPIC
#a1.sinks.k1.maxMessageSize=50000000
###-------------------------------------------------#

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 10000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1

